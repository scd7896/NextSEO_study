robots.txt 파일을 사용해서 크롤링을 제어하는 방법
기본적으로 robots.txt 파일은 사이트의 루트 디렉토리에 저장해 둡니다.
예를 들면 브라우저에 http://www.seo-korea.com/robots.txt 를 입력하면 파일을 읽을수 있어야 합니다.
http://www.seo-korea.com/myfolder/robots.txt 는 유효하지 않습니다.
검색엔진의 크롤러로 부터 모든 문서수집을 차단하려면 아래와 같은 내용을 robots.txt에 삽입합니다.
User-agent: *
Disallow: /

반대로 모든 문서를 허용하려면
User-agent: *
Disallow:

특정 폴더만을 차단하려면 
User-agent: *
Disallow: /members/
Disallow: /search/
Disallow: /images/

특정 검색엔진 크롤러를 차단하려면 (구글만 차단)
User-agent: Googlebot
Disallow: /

또는 특정 검색엔진 크롤러만 허용하려면 (구글만 허용)
User-agent: Google
Disallow:
User-agent: *
Disallow: /

특정 페이지를 차단하려면
User-agent: *
Disallow: /members/personal_info.html

robots.txt 파일에 사이트맵 명시
User-agent: *
Disallow:
Sitemap: http://www.seo-korea.com/sitemap.xml